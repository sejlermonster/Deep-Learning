\chapter{Exercise 1}
\label{chp:e1}
\section{Forward pass}

\section{Backward pass}

\section{Training}
For training stochastic gradient decent is already implemented. Similar to gradient descent it tries to minimize the loss function but it does so by only looking at a subset of the training data

 This is a stochastic implementation of gradient descent. 

In this exercise it implementation is extended by implementing momentum

In each step at gradient descen a step is taken in the direction negative of the graident

We want the velocity to die off slowly 


Momentum is a way of "damping" oscillations in directions of high curvature by combining gradients with opposite signs


Velocity vector      after 

In the Listing in \ref{lst:momentum} the momentum is updated. T
\begin{lstlisting}[language=Python, label=lst:momentum, caption=Momentum update]
dx = momentum * self.step_cache[p] - learning_rate * grads[p] 
self.step_cache[p] = dx 
\end{lstlisting}




\begin{lstlisting}[language=Python, label=lst:RMSprop, caption=RMSProp update]
self.step_cache[p] = decay_rate * self.step_cache[p] + (1 - decay_rate) * grads[p]**2
dx = - learning_rate * grads[p] / (np.sqrt(self.step_cache[p]) + eps)
\end{lstlisting}


    